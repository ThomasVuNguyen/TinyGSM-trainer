{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TinyGSM Fine-tuning Notebook\n",
        "\n",
        "This notebook demonstrates how to fine-tune language models using Unsloth for efficient training on GTX 1050 Ti and similar hardware."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2mUsing Python 3.12.3 environment at: myenv\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m2 packages\u001b[0m \u001b[2min 14ms\u001b[0m\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "uv pip install unsloth transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GTX 1050 Ti compatibility fixes - disable compilation\n",
        "import unsloth\n",
        "import os\n",
        "import csv\n",
        "import json\n",
        "from datetime import datetime\n",
        "from transformers import TrainerCallback\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration Loading\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuration loaded successfully!\n",
            "Model: unsloth/gemma-3-270m-it\n",
            "Dataset: ThomasTheMaker/tulu-3-sft-personas-algebra\n",
            "Max Steps: -1\n",
            "Learning Rate: 5e-05\n"
          ]
        }
      ],
      "source": [
        "# Load configuration from JSON file\n",
        "with open('algebra.json', 'r') as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "# Extract configuration sections\n",
        "model_config = config['model_config']\n",
        "dataset_config = config['dataset_config']\n",
        "lora_config = config['lora_config']\n",
        "training_config = config['training_config']\n",
        "inference_config = config['inference_config']\n",
        "saving_config = config['saving_config']\n",
        "logging_config = config['logging_config']\n",
        "\n",
        "# Model Configuration\n",
        "HUB_MODEL_NAME = model_config['hub_model_name']\n",
        "MODEL_NAME = model_config['base_model_name']\n",
        "MAX_SEQ_LENGTH = model_config['max_seq_length']\n",
        "LOAD_IN_4BIT = model_config['load_in_4bit']\n",
        "LOAD_IN_8BIT = model_config['load_in_8bit']\n",
        "FULL_FINETUNING = model_config['full_finetuning']\n",
        "\n",
        "# Dataset Configuration\n",
        "DATASET_NAME = dataset_config['dataset_name']\n",
        "DATASET_SPLIT = dataset_config['dataset_split']\n",
        "CHAT_TEMPLATE = dataset_config['chat_template']\n",
        "\n",
        "# LoRA Configuration\n",
        "LORA_R = lora_config['r']\n",
        "LORA_ALPHA = LORA_R * lora_config['alpha_multiplier']\n",
        "LORA_DROPOUT = lora_config['dropout']\n",
        "LORA_BIAS = lora_config['bias']\n",
        "USE_GRADIENT_CHECKPOINTING = lora_config['use_gradient_checkpointing']\n",
        "RANDOM_STATE = lora_config['random_state']\n",
        "USE_RSLORA = lora_config['use_rslora']\n",
        "LOFTQ_CONFIG = lora_config['loftq_config']\n",
        "TARGET_MODULES = lora_config['target_modules']\n",
        "\n",
        "# Training Configuration\n",
        "PER_DEVICE_TRAIN_BATCH_SIZE = training_config['per_device_train_batch_size']\n",
        "GRADIENT_ACCUMULATION_STEPS = training_config['gradient_accumulation_steps']\n",
        "WARMUP_STEPS = training_config['warmup_steps']\n",
        "MAX_STEPS = training_config['max_steps']\n",
        "NUM_TRAIN_EPOCHS = training_config['num_train_epochs']\n",
        "LEARNING_RATE = training_config['learning_rate']\n",
        "WEIGHT_DECAY = training_config['weight_decay']\n",
        "LR_SCHEDULER_TYPE = training_config['lr_scheduler_type']\n",
        "SEED = training_config['seed']\n",
        "OUTPUT_DIR = training_config['output_dir']\n",
        "REPORT_TO = training_config['report_to']\n",
        "OPTIM = training_config['optim']\n",
        "LOGGING_STEPS = training_config['logging_steps']\n",
        "\n",
        "# Inference Configuration\n",
        "MAX_NEW_TOKENS = inference_config['max_new_tokens']\n",
        "TEMPERATURE = inference_config['temperature']\n",
        "TOP_P = inference_config['top_p']\n",
        "TOP_K = inference_config['top_k']\n",
        "DO_SAMPLE = inference_config['do_sample']\n",
        "\n",
        "# Model Saving Configuration\n",
        "SAVE_LOCAL = saving_config['save_local']\n",
        "SAVE_16BIT = saving_config['save_16bit']\n",
        "SAVE_4BIT = saving_config['save_4bit']\n",
        "SAVE_LORA = saving_config['save_lora']\n",
        "PUSH_TO_HUB = saving_config['push_to_hub']\n",
        "\n",
        "# CSV Logging Configuration\n",
        "CSV_LOG_ENABLED = logging_config['csv_log_enabled']\n",
        "CSV_LOG_FILE = f\"{HUB_MODEL_NAME}/training_metrics.csv\"\n",
        "\n",
        "print(\"Configuration loaded successfully!\")\n",
        "print(f\"Model: {MODEL_NAME}\")\n",
        "print(f\"Dataset: {DATASET_NAME}\")\n",
        "print(f\"Max Steps: {MAX_STEPS}\")\n",
        "print(f\"Learning Rate: {LEARNING_RATE}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Available Models Reference\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available models:\n",
            "  - unsloth/gemma-3-1b-it-unsloth-bnb-4bit\n",
            "  - unsloth/gemma-3-4b-it-unsloth-bnb-4bit\n",
            "  - unsloth/gemma-3-12b-it-unsloth-bnb-4bit\n",
            "  - unsloth/gemma-3-27b-it-unsloth-bnb-4bit\n",
            "  - unsloth/Llama-3.1-8B\n",
            "  - unsloth/Llama-3.2-3B\n",
            "  - unsloth/Llama-3.3-70B\n",
            "  - unsloth/mistral-7b-instruct-v0.3\n",
            "  - unsloth/Phi-4\n"
          ]
        }
      ],
      "source": [
        "# Available Models (for reference)\n",
        "FOURBIT_MODELS = [\n",
        "    # 4bit dynamic quants for superior accuracy and low memory use\n",
        "    \"unsloth/gemma-3-1b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/gemma-3-27b-it-unsloth-bnb-4bit\",\n",
        "    # Other popular models!\n",
        "    \"unsloth/Llama-3.1-8B\",\n",
        "    \"unsloth/Llama-3.2-3B\",\n",
        "    \"unsloth/Llama-3.3-70B\",\n",
        "    \"unsloth/mistral-7b-instruct-v0.3\",\n",
        "    \"unsloth/Phi-4\",\n",
        "]  # More models at https://huggingface.co/unsloth\n",
        "\n",
        "print(\"Available models:\")\n",
        "for model in FOURBIT_MODELS:\n",
        "    print(f\"  - {model}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. CUDA Setup\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment setup complete!\n",
            "PyTorch version: 2.8.0+cu128\n",
            "CUDA available: True\n",
            "CUDA device: NVIDIA GeForce RTX 4090\n"
          ]
        }
      ],
      "source": [
        "# Load environment variables from .env file (optional)\n",
        "# try:\n",
        "#     from dotenv import load_dotenv\n",
        "#     load_dotenv()\n",
        "#     print(\"Environment variables loaded from .env file\")\n",
        "# except ImportError:\n",
        "#     print(\"python-dotenv not installed. Using system environment variables only.\")\n",
        "#     print(\"To install: pip install python-dotenv\")\n",
        "\n",
        "# Set CUDA environment variables for GTX 1050 Ti compatibility\n",
        "os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\"\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
        "os.environ[\"TORCH_INDUCTOR\"] = \"0\"\n",
        "os.environ[\"TORCHINDUCTOR_MAX_AUTOTUNE\"] = \"0\"\n",
        "os.environ[\"TORCH_COMPILE_DISABLE\"] = \"1\"\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "from unsloth import FastModel\n",
        "import torch\n",
        "\n",
        "# Disable Triton and dynamic compilation\n",
        "torch._dynamo.config.suppress_errors = True\n",
        "torch._dynamo.reset()\n",
        "torch._dynamo.config.disable = True\n",
        "\n",
        "print(\"Environment setup complete!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. CSV Logging Callback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CSV Metrics Callback class defined!\n"
          ]
        }
      ],
      "source": [
        "class CSVMetricsCallback(TrainerCallback):\n",
        "    \"\"\"Callback to log training metrics to CSV file\"\"\"\n",
        "    \n",
        "    def __init__(self, csv_file_path):\n",
        "        self.csv_file_path = csv_file_path\n",
        "        self.metrics_data = []\n",
        "        self.fieldnames = ['step', 'epoch', 'loss', 'grad_norm', 'learning_rate', 'timestamp']\n",
        "        \n",
        "        # Create CSV file with headers\n",
        "        with open(csv_file_path, 'w', newline='') as csvfile:\n",
        "            writer = csv.DictWriter(csvfile, fieldnames=self.fieldnames)\n",
        "            writer.writeheader()\n",
        "    \n",
        "    def on_log(self, args, state, control, model=None, logs=None, **kwargs):\n",
        "        \"\"\"Called when trainer logs metrics\"\"\"\n",
        "        if logs is not None and CSV_LOG_ENABLED:\n",
        "            # Extract metrics from logs\n",
        "            metrics = {\n",
        "                'step': state.global_step,\n",
        "                'epoch': logs.get('epoch', 0),\n",
        "                'loss': logs.get('loss', None),\n",
        "                'grad_norm': logs.get('grad_norm', None),\n",
        "                'learning_rate': logs.get('learning_rate', None),\n",
        "                'timestamp': datetime.now().isoformat()\n",
        "            }\n",
        "            \n",
        "            # Only log if we have meaningful data\n",
        "            if any(v is not None for v in [metrics['loss'], metrics['grad_norm'], metrics['learning_rate']]):\n",
        "                self.metrics_data.append(metrics)\n",
        "                \n",
        "                # Write to CSV file\n",
        "                with open(self.csv_file_path, 'a', newline='') as csvfile:\n",
        "                    writer = csv.DictWriter(csvfile, fieldnames=self.fieldnames)\n",
        "                    writer.writerow(metrics)\n",
        "                \n",
        "                print(f\"Logged metrics: Step {metrics['step']}, Loss: {metrics['loss']}, LR: {metrics['learning_rate']}\")\n",
        "    \n",
        "    def on_train_end(self, args, state, control, **kwargs):\n",
        "        \"\"\"Called when training ends\"\"\"\n",
        "        if CSV_LOG_ENABLED:\n",
        "            print(f\"\\nTraining metrics saved to: {self.csv_file_path}\")\n",
        "            print(f\"Total logged entries: {len(self.metrics_data)}\")\n",
        "\n",
        "print(\"CSV Metrics Callback class defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Model Loading\n",
        "\n",
        "Load the pre-trained model and configure LoRA adapters for efficient fine-tuning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth 2025.9.4: Fast Gemma3_Text patching. Transformers: 4.56.1.\n",
            "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 23.516 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Model loaded: unsloth/gemma-3-270m-it\n",
            "Max sequence length: 1024\n",
            "Load in 4bit: False\n",
            "Load in 8bit: True\n",
            "Full finetuning: False\n"
          ]
        }
      ],
      "source": [
        "model, tokenizer = FastModel.from_pretrained(\n",
        "    model_name=MODEL_NAME,\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    load_in_4bit=LOAD_IN_4BIT,\n",
        "    load_in_8bit=LOAD_IN_8BIT,\n",
        "    full_finetuning=FULL_FINETUNING,\n",
        "    # token = \"hf_...\", # use one if using gated models\n",
        ")\n",
        "\n",
        "print(f\"Model loaded: {MODEL_NAME}\")\n",
        "print(f\"Max sequence length: {MAX_SEQ_LENGTH}\")\n",
        "print(f\"Load in 4bit: {LOAD_IN_4BIT}\")\n",
        "print(f\"Load in 8bit: {LOAD_IN_8BIT}\")\n",
        "print(f\"Full finetuning: {FULL_FINETUNING}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Add LoRA Adapters\n",
        "\n",
        "We now add LoRA adapters so we only need to update a small amount of parameters!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Making `model.base_model.model.model` require gradients\n",
            "LoRA adapters added successfully!\n",
            "LoRA rank (r): 128\n",
            "LoRA alpha: 256\n",
            "LoRA dropout: 0\n",
            "Target modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']\n"
          ]
        }
      ],
      "source": [
        "model = FastModel.get_peft_model(\n",
        "    model,\n",
        "    r=LORA_R,\n",
        "    target_modules=TARGET_MODULES,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    bias=LORA_BIAS,\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing=USE_GRADIENT_CHECKPOINTING,\n",
        "    random_state=RANDOM_STATE,\n",
        "    use_rslora=USE_RSLORA,\n",
        "    loftq_config=LOFTQ_CONFIG,\n",
        ")\n",
        "\n",
        "print(\"LoRA adapters added successfully!\")\n",
        "print(f\"LoRA rank (r): {LORA_R}\")\n",
        "print(f\"LoRA alpha: {LORA_ALPHA}\")\n",
        "print(f\"LoRA dropout: {LORA_DROPOUT}\")\n",
        "print(f\"Target modules: {TARGET_MODULES}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Data Preparation\n",
        "\n",
        "We now use the `Gemma-3` format for conversation style finetunes. We use the reformatted [Tulu-3 SFT Personas Instruction Following](https://huggingface.co/datasets/ThomasTheMaker/tulu-3-sft-personas-instruction-following) dataset. Gemma-3 renders multi turn conversations like below:\n",
        "\n",
        "```\n",
        "<bos><start_of_turn>user\n",
        "Hello!<end_of_turn>\n",
        "<start_of_turn>model\n",
        "Hey there!<end_of_turn>\n",
        "```\n",
        "\n",
        "We use our `get_chat_template` function to get the correct chat template. We support `zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, phi3, llama3, phi4, qwen2.5, gemma3` and more.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template=CHAT_TEMPLATE,\n",
        ")\n",
        "\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(DATASET_NAME, split=DATASET_SPLIT)\n",
        "\n",
        "print(f\"Dataset loaded: {DATASET_NAME}\")\n",
        "print(f\"Dataset split: {DATASET_SPLIT}\")\n",
        "print(f\"Chat template: {CHAT_TEMPLATE}\")\n",
        "print(f\"Dataset size: {len(dataset)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Convert Dataset to ChatML Format\n",
        "\n",
        "We now use `convert_to_chatml` to convert the reformatted dataset (with input/output/system columns) to the correct format for finetuning purposes!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convert_to_chatml(example):\n",
        "    return {\n",
        "        \"conversations\": [\n",
        "            {\"role\": \"system\", \"content\": example[\"system\"]},\n",
        "            {\"role\": \"user\", \"content\": example[\"input\"]},\n",
        "            {\"role\": \"assistant\", \"content\": example[\"output\"]}\n",
        "        ]\n",
        "    }\n",
        "\n",
        "dataset = dataset.map(\n",
        "    convert_to_chatml\n",
        ")\n",
        "\n",
        "print(\"Dataset converted to ChatML format\")\n",
        "print(\"Sample conversation:\")\n",
        "print(dataset[100])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Apply Chat Template\n",
        "\n",
        "We now have to apply the chat template for `Gemma3` onto the conversations, and save it to `text`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def formatting_prompts_func(examples):\n",
        "   convos = examples[\"conversations\"]\n",
        "   texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False).removeprefix('<bos>') for convo in convos]\n",
        "   return { \"text\" : texts, }\n",
        "\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True)\n",
        "\n",
        "print(\"Chat template applied successfully!\")\n",
        "print(\"Sample formatted text:\")\n",
        "print(dataset[100]['text'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Training Setup\n",
        "\n",
        "Now let's set up the training configuration and trainer. We do 100 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "# Create model directory and copy config\n",
        "import shutil\n",
        "os.makedirs(HUB_MODEL_NAME, exist_ok=True)\n",
        "\n",
        "# Copy the JSON config to the model folder for reproducibility\n",
        "config_copy_path = f\"{HUB_MODEL_NAME}/config.json\"\n",
        "shutil.copy2('algebra.json', config_copy_path)\n",
        "print(f\"Configuration copied to: {config_copy_path}\")\n",
        "\n",
        "# Initialize CSV logging callback\n",
        "csv_callback = None\n",
        "if CSV_LOG_ENABLED:\n",
        "    csv_callback = CSVMetricsCallback(CSV_LOG_FILE)\n",
        "    print(f\"CSV logging enabled. Metrics will be saved to: {CSV_LOG_FILE}\")\n",
        "\n",
        "print(\"Training setup complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare training arguments - handle max_steps properly\n",
        "training_args = {\n",
        "    \"dataset_text_field\": \"text\",\n",
        "    \"per_device_train_batch_size\": PER_DEVICE_TRAIN_BATCH_SIZE,\n",
        "    \"gradient_accumulation_steps\": GRADIENT_ACCUMULATION_STEPS,\n",
        "    \"warmup_steps\": WARMUP_STEPS,\n",
        "    \"learning_rate\": LEARNING_RATE,\n",
        "    \"logging_steps\": LOGGING_STEPS,\n",
        "    \"optim\": OPTIM,\n",
        "    \"weight_decay\": WEIGHT_DECAY,\n",
        "    \"lr_scheduler_type\": LR_SCHEDULER_TYPE,\n",
        "    \"seed\": SEED,\n",
        "    \"output_dir\": OUTPUT_DIR,\n",
        "    \"report_to\": REPORT_TO,\n",
        "}\n",
        "\n",
        "# Add either max_steps OR num_train_epochs, not both\n",
        "if MAX_STEPS and MAX_STEPS > 0:\n",
        "    training_args[\"max_steps\"] = MAX_STEPS\n",
        "else:\n",
        "    training_args[\"num_train_epochs\"] = NUM_TRAIN_EPOCHS\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    eval_dataset=None,  # Can set up evaluation!\n",
        "    args=SFTConfig(**training_args),\n",
        ")\n",
        "\n",
        "# Add CSV callback to trainer\n",
        "if csv_callback:\n",
        "    trainer.add_callback(csv_callback)\n",
        "\n",
        "print(\"Trainer created successfully!\")\n",
        "print(f\"Training arguments: {training_args}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Configure Response-Only Training\n",
        "\n",
        "We also use Unsloth's `train_on_completions` method to only train on the assistant outputs and ignore the loss on the user's inputs. This helps increase accuracy of finetunes!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import train_on_responses_only\n",
        "trainer = train_on_responses_only(\n",
        "    trainer,\n",
        "    instruction_part = \"<start_of_turn>user\\n\",\n",
        "    response_part = \"<start_of_turn>model\\n\",\n",
        ")\n",
        "\n",
        "print(\"Response-only training configured!\")\n",
        "print(\"Sample input IDs:\")\n",
        "print(tokenizer.decode(trainer.train_dataset[100][\"input_ids\"]))\n",
        "print(\"\\nSample labels (only assistant responses):\")\n",
        "print(tokenizer.decode([tokenizer.pad_token_id if x == -100 else x for x in trainer.train_dataset[100][\"labels\"]]).replace(tokenizer.pad_token, \" \"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Memory Check and Training\n",
        "\n",
        "Check current memory usage before starting training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Start Training\n",
        "\n",
        "Let's train the model! To resume a training run, set `trainer.train(resume_from_checkpoint = True)`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()\n",
        "\n",
        "print(\"Training completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training Results\n",
        "\n",
        "Show final memory and time stats after training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### CSV Metrics Summary\n",
        "\n",
        "Display training metrics summary from CSV logging.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show CSV metrics summary\n",
        "if CSV_LOG_ENABLED and csv_callback and csv_callback.metrics_data:\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"TRAINING METRICS SUMMARY\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    # Calculate summary statistics\n",
        "    losses = [m['loss'] for m in csv_callback.metrics_data if m['loss'] is not None]\n",
        "    learning_rates = [m['learning_rate'] for m in csv_callback.metrics_data if m['learning_rate'] is not None]\n",
        "    grad_norms = [m['grad_norm'] for m in csv_callback.metrics_data if m['grad_norm'] is not None]\n",
        "    \n",
        "    if losses:\n",
        "        print(f\"Final Loss: {losses[-1]:.4f}\")\n",
        "        print(f\"Initial Loss: {losses[0]:.4f}\")\n",
        "        print(f\"Loss Reduction: {((losses[0] - losses[-1]) / losses[0] * 100):.2f}%\")\n",
        "        print(f\"Min Loss: {min(losses):.4f}\")\n",
        "        print(f\"Max Loss: {max(losses):.4f}\")\n",
        "    \n",
        "    if learning_rates:\n",
        "        print(f\"Final Learning Rate: {learning_rates[-1]:.2e}\")\n",
        "        print(f\"Initial Learning Rate: {learning_rates[0]:.2e}\")\n",
        "    \n",
        "    if grad_norms:\n",
        "        print(f\"Final Gradient Norm: {grad_norms[-1]:.4f}\")\n",
        "        print(f\"Average Gradient Norm: {sum(grad_norms)/len(grad_norms):.4f}\")\n",
        "    \n",
        "    print(f\"Total Logged Steps: {len(csv_callback.metrics_data)}\")\n",
        "    print(f\"CSV File: {CSV_LOG_FILE}\")\n",
        "    print(\"=\"*50)\n",
        "else:\n",
        "    print(\"CSV logging not enabled or no metrics available.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Inference\n",
        "\n",
        "Let's run the model via Unsloth native inference! According to the `Gemma-3` team, the recommended settings for inference are `temperature = 1.0, top_p = 0.95, top_k = 64`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {'role': 'system','content':dataset['conversations'][10][0]['content']},\n",
        "    {\"role\" : 'user', 'content' : dataset['conversations'][10][1]['content']}\n",
        "]\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = False,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        ").removeprefix('<bos>')\n",
        "\n",
        "print(\"Input prompt:\")\n",
        "print(text)\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Generated response:\")\n",
        "print(\"=\"*50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import TextStreamer\n",
        "# Fix cache compatibility issue by using a different generation approach\n",
        "inputs = tokenizer(text, return_tensors = \"pt\").to(\"cuda\")\n",
        "outputs = model.generate(\n",
        "    input_ids=inputs[\"input_ids\"],\n",
        "    attention_mask=inputs[\"attention_mask\"],\n",
        "    max_new_tokens=MAX_NEW_TOKENS,\n",
        "    temperature=TEMPERATURE,\n",
        "    top_p=TOP_P,\n",
        "    top_k=TOP_K,\n",
        "    do_sample=DO_SAMPLE,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    use_cache=False,  # Disable cache to avoid compatibility issues\n",
        ")\n",
        "\n",
        "# Decode and print the generated text\n",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(generated_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Model Saving\n",
        "\n",
        "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
        "\n",
        "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Local saving\n",
        "if SAVE_LOCAL:\n",
        "    model.save_pretrained(HUB_MODEL_NAME)\n",
        "    tokenizer.save_pretrained(HUB_MODEL_NAME)\n",
        "    print(f\"Model and tokenizer saved locally to {HUB_MODEL_NAME}\")\n",
        "\n",
        "# Get Hugging Face token from environment\n",
        "hf_token = os.getenv(\"HF_TOKEN\")\n",
        "if PUSH_TO_HUB and hf_token:\n",
        "    model.push_to_hub(HUB_MODEL_NAME, token=hf_token)\n",
        "    tokenizer.push_to_hub(HUB_MODEL_NAME, token=hf_token)\n",
        "    print(\"Model and tokenizer uploaded to Hugging Face Hub\")\n",
        "elif PUSH_TO_HUB and not hf_token:\n",
        "    print(\"Warning: HF_TOKEN not found in environment variables. Skipping Hugging Face upload.\")\n",
        "\n",
        "print(\"Model saving completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loading Saved LoRA Adapters\n",
        "\n",
        "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if False:\n",
        "    from unsloth import FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"gemma-3-270m-tulu-3-sft-personas-instruction-following\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = 2048,\n",
        "        load_in_4bit = False,\n",
        "    )\n",
        "    print(\"LoRA adapters loaded for inference\")\n",
        "else:\n",
        "    print(\"LoRA adapter loading skipped (set to False)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Saving to Different Formats\n",
        "\n",
        "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Merge to 16bit\n",
        "if SAVE_16BIT:\n",
        "    model.save_pretrained_merged(f\"{HUB_MODEL_NAME}-16bit\", tokenizer, save_method=\"merged_16bit\")\n",
        "    if PUSH_TO_HUB and hf_token:\n",
        "        model.push_to_hub_merged(f\"{HUB_MODEL_NAME}-16bit\", tokenizer, save_method=\"merged_16bit\", token=hf_token)\n",
        "        print(\"16-bit merged model uploaded to Hugging Face Hub\")\n",
        "    elif PUSH_TO_HUB and not hf_token:\n",
        "        print(\"Warning: HF_TOKEN not found. Skipping 16-bit model upload to Hugging Face.\")\n",
        "    else:\n",
        "        print(\"16-bit model saved locally\")\n",
        "\n",
        "# Merge to 4bit\n",
        "if SAVE_4BIT:\n",
        "    model.save_pretrained_merged(f\"{HUB_MODEL_NAME}-4bit\", tokenizer, save_method=\"merged_4bit\")\n",
        "    if PUSH_TO_HUB and hf_token:\n",
        "        model.push_to_hub_merged(f\"{HUB_MODEL_NAME}-4bit\", tokenizer, save_method=\"merged_4bit\", token=hf_token)\n",
        "        print(\"4-bit merged model uploaded to Hugging Face Hub\")\n",
        "    elif PUSH_TO_HUB and not hf_token:\n",
        "        print(\"Warning: HF_TOKEN not found. Skipping 4-bit model upload to Hugging Face.\")\n",
        "    else:\n",
        "        print(\"4-bit model saved locally\")\n",
        "\n",
        "# Just LoRA adapters\n",
        "if SAVE_LORA:\n",
        "    model.save_pretrained(f\"{HUB_MODEL_NAME}-lora\")\n",
        "    tokenizer.save_pretrained(f\"{HUB_MODEL_NAME}-lora\")\n",
        "    if PUSH_TO_HUB and hf_token:\n",
        "        model.push_to_hub(f\"{HUB_MODEL_NAME}-lora\", token=hf_token)\n",
        "        tokenizer.push_to_hub(f\"{HUB_MODEL_NAME}-lora\", token=hf_token)\n",
        "        print(\"LoRA adapters uploaded to Hugging Face Hub\")\n",
        "    else:\n",
        "        print(\"LoRA adapters saved locally\")\n",
        "\n",
        "print(\"All model saving operations completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Summary\n",
        "\n",
        "This notebook has successfully:\n",
        "\n",
        "1. ✅ Loaded configuration from JSON\n",
        "2. ✅ Set up CUDA environment for GTX 1050 Ti compatibility\n",
        "3. ✅ Loaded and configured the model with LoRA adapters\n",
        "4. ✅ Prepared the dataset with proper chat templates\n",
        "5. ✅ Set up training with response-only learning\n",
        "6. ✅ Trained the model with CSV logging\n",
        "7. ✅ Performed inference testing\n",
        "8. ✅ Saved the model in multiple formats\n",
        "\n",
        "The fine-tuning process is now complete! You can use the saved models for inference or further training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
